# Notebook 1: Sales Data Processing
# Cleaned version â€“ Secrets removed

from azure.storage.filedatalake import DataLakeServiceClient
import os
from pyspark.sql import SparkSession

# ------------------------------
# Set up Spark
# ------------------------------
spark = SparkSession.builder.appName("SalesDataProcessing").getOrCreate()

# ------------------------------
# Connect to Azure Data Lake (ADLS)
# ------------------------------
# Use environment variable instead of hardcoding secret
connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")

service_client = DataLakeServiceClient.from_connection_string(connection_string)
fs_client = service_client.get_file_system_client("gold")  # container name

# ------------------------------
# Create target folder if it doesn't exist
# ------------------------------
try:
    fs_client.create_directory("sales_summary")
    print("Folder 'sales_summary' created.")
except:
    print("Folder may already exist.")

# ------------------------------
# Move Delta files to sales_summary/
# ------------------------------
for file in fs_client.get_paths(path=""):
    if not file.is_directory and file.name.startswith("part-") and file.name.endswith(".delta"):
        src = fs_client.get_file_client(file.name)
        dest = fs_client.get_file_client(f"sales_summary/{file.name}")
        dest.create_file()
        dest.append_data(src.download_file().readall(), 0)
        dest.flush_data(src.get_file_properties().size)
        src.delete_file()
        print(f"Moved {file.name}")

# Move _delta_log folder
try:
    fs_client.rename_directory("_delta_log", "sales_summary/_delta_log")
    print("_delta_log folder moved successfully")
except:
    print("_delta_log already moved or error")

# ------------------------------
# List files in sales_summary/
# ------------------------------
paths = fs_client.get_paths(path="sales_summary")
for path in paths:
    print(path.name, "(Directory)" if path.is_directory else "(File)")

# ------------------------------
# Read Delta table in Spark
# ------------------------------
df = spark.read.format("delta").load(
    "abfss://gold@<your_storage_account>.dfs.core.windows.net/sales_summary"
)

df.show()

# ------------------------------
# Save as Parquet
# ------------------------------
df.write.mode("overwrite").parquet(
    "abfss://gold@<your_storage_account>.dfs.core.windows.net/sales_summary_parquet"
)
