# Notebook 2: Sales Ingestion
# Cleaned version â€“ Secrets removed

from azure.storage.filedatalake import DataLakeServiceClient
import os
from pyspark.sql import SparkSession

# ------------------------------
# Set up Spark
# ------------------------------
spark = SparkSession.builder.appName("SalesDataIngestion").getOrCreate()

# ------------------------------
# Connect to Azure Data Lake (ADLS)
# ------------------------------
# Use environment variable instead of hardcoding secret
connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")

service_client = DataLakeServiceClient.from_connection_string(connection_string)
fs_client = service_client.get_file_system_client("bronze")  # container name

# ------------------------------
# Create target folder if it doesn't exist
# ------------------------------
try:
    fs_client.create_directory("sales_raw")
    print("Folder 'sales_raw' created.")
except:
    print("Folder may already exist.")

# ------------------------------
# Upload CSV files to ADLS
# ------------------------------
local_csv_folder = "data/raw/"  # local folder containing CSVs

for csv_file in os.listdir(local_csv_folder):
    if csv_file.endswith(".csv"):
        file_client = fs_client.get_file_client(f"sales_raw/{csv_file}")
        file_client.create_file()
        with open(os.path.join(local_csv_folder, csv_file), "rb") as f:
            file_client.append_data(f.read(), 0)
            file_client.flush_data(os.path.getsize(os.path.join(local_csv_folder, csv_file)))
        print(f"Uploaded {csv_file} to ADLS")

# ------------------------------
# List files in sales_raw/
# ------------------------------
paths = fs_client.get_paths(path="sales_raw")
for path in paths:
    print(path.name, "(Directory)" if path.is_directory else "(File)")

# ------------------------------
# Read CSVs into Spark
# ------------------------------
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("abfss://bronze@<your_storage_account>.dfs.core.windows.net/sales_raw/*.csv")

df.show()

# ------------------------------
# Save as Delta Table
# ------------------------------
df.write.format("delta").mode("overwrite").save(
    "abfss://silver@<your_storage_account>.dfs.core.windows.net/sales_delta"
)
